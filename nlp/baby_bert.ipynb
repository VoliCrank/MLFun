{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "56726162-bce3-464a-83c7-e9290006eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_dataset\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a045c16-a608-473a-ac07-4f26e7a59f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = os.path.dirname(mycwd)+ '/data/aclImdb/train/pos/'\n",
    "neg_path = os.path.dirname(mycwd)+ '/data/aclImdb/train/neg/'\n",
    "#pos_path = 'E:/MLFun/data/aclImdb_v1/aclImdb/train/pos/'\n",
    "#neg_path = 'E:/MLFun/data/aclImdb_v1/aclImdb/train/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90adde2a-bbf0-403d-8e2e-3deead52e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "with open(pos_path + '0_9.txt') as f:\n",
    "    print(f.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae14c0cb-4179-4bc2-94a3-9e68924f2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans all the <br>'s for now. TODO: clean '\\' as well\n",
    "def clean(text):\n",
    "    ret = re.sub(r\"\\<br..\\>\", ' ', text)\n",
    "    return ret\n",
    "\n",
    "# get all txt files from given path and append appropriate training value\n",
    "def get_data(data_path, val):\n",
    "    ret = []\n",
    "    files = glob.glob(data_path + \"*.txt\")\n",
    "    for fle in files:\n",
    "        with open(fle, encoding=\"utf8\") as f:\n",
    "            text = f.readlines()[0]\n",
    "            text = clean(text)\n",
    "            ret.append([text,val])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4016771a-8445-4c50-b0b8-4540675de5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_train += get_data(pos_path, 1) + get_data(neg_path, 0)\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7e812cd1-43ba-44ea-9d58-78b68260b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X_train)\n",
    "temp = X_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a7c7744-1539-4411-8204-92e3df58adc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f992f647-f515-4851-8ac9-0fda9848c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(x, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Create embedding vectors for the input articles \n",
    "    \n",
    "    Args:\n",
    "        articles (list(dict)): \n",
    "            A list of articles \n",
    "    \n",
    "    Returns: \n",
    "        (numpy.ndarray or other format):\n",
    "            An MxN matrix/table for the article embeddings, where \n",
    "            each row is for the embedding vector for an article\n",
    "    \"\"\"\n",
    "    text  = x\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    #model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    #model.eval()\n",
    "    sentence_emb = torch.LongTensor(tokenizer.encode(text, truncation = True))\n",
    "    sentence_emb = sentence_emb.to(device)\n",
    "    with torch.no_grad():\n",
    "        # embed the sentences\n",
    "        out = model(sentence_emb.unsqueeze(0))\n",
    "        hidden_states = out[2]\n",
    "            \n",
    "    # sum up last four layers for improved performance\n",
    "    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "        \n",
    "    # reshape the embedding to (1,768)\n",
    "    sum_sentence_embedding = torch.mean(sum(last_four_layers), dim=1).squeeze()\n",
    "    ret = np.array(sum_sentence_embedding.cpu())\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90015ba1-e46c-4ff0-a508-1834e1b3494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25b610f8-4c42-4a6a-80d9-7affadd7f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_emb = torch.LongTensor(tokenizer.encode(temp[0][0], truncation = True))\n",
    "sentence_emb = sentence_emb.to(device)\n",
    "with torch.no_grad():\n",
    "    # embed the sentences\n",
    "    out = model(sentence_emb.unsqueeze(0))\n",
    "    hidden_states = out[2]\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "        \n",
    "# reshape the embedding to (1,768)\n",
    "sum_sentence_embedding = torch.mean(sum(last_four_layers), dim=1).squeeze()\n",
    "ret = np.array(sum_sentence_embedding.cpu())\n",
    "#ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6ac4ddd-7efe-4ded-ae09-60bd59f1ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "df = pd.DataFrame(temp)\n",
    "df.rename(columns={0: 'text', 1: 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "352d4bf5-8d3b-48a6-815d-214c48cba29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df.apply(lambda x: embed_texts(x['text'], model, tokenizer), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20078226-96f4-470f-8811-95c1f37e0b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1',\n",
       "       '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0',\n",
       "       '1', '1', '0', '1', '1', '0', '1', '0', '0', '1', '1', '0', '1',\n",
       "       '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0',\n",
       "       '0', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '1', '0',\n",
       "       '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1',\n",
       "       '1', '1', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0',\n",
       "       '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '1', '1', '1',\n",
       "       '0', '1', '1', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0',\n",
       "       '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0',\n",
       "       '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '1', '1', '0',\n",
       "       '1', '0', '1', '1', '1', '0', '0', '0', '0', '1', '0', '0', '0',\n",
       "       '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '0',\n",
       "       '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0',\n",
       "       '1', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1',\n",
       "       '0', '1', '0', '0', '1', '1', '1', '1', '0', '1', '0', '1', '0',\n",
       "       '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1',\n",
       "       '1', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1',\n",
       "       '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1',\n",
       "       '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '1',\n",
       "       '0', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0',\n",
       "       '1', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0', '1', '1',\n",
       "       '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '1',\n",
       "       '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1',\n",
       "       '1', '1', '1', '1', '0', '0', '0', '0', '1', '0', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '0',\n",
       "       '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0',\n",
       "       '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1',\n",
       "       '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1',\n",
       "       '0', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '1',\n",
       "       '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0',\n",
       "       '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1',\n",
       "       '1', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1',\n",
       "       '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1',\n",
       "       '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0',\n",
       "       '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0',\n",
       "       '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '1',\n",
       "       '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '1', '1', '1',\n",
       "       '0', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0',\n",
       "       '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '1', '1',\n",
       "       '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '1', '0',\n",
       "       '0', '1', '0', '1', '1', '1', '1', '0', '0', '1', '0', '0', '0',\n",
       "       '1', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0', '1', '0',\n",
       "       '0', '1', '0', '1', '0', '0', '1', '1', '1', '0', '1', '0', '1',\n",
       "       '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '1', '1',\n",
       "       '0', '1', '0', '0', '1', '1', '1', '0', '1', '0', '0', '1', '1',\n",
       "       '1', '0', '0', '0', '1', '1', '1', '0', '1', '0', '1', '1', '1',\n",
       "       '0', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '1', '0',\n",
       "       '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0',\n",
       "       '0', '1', '1', '1', '1', '0', '0', '1', '0', '0', '0', '1', '1',\n",
       "       '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '1',\n",
       "       '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '1', '0', '1',\n",
       "       '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '0', '1',\n",
       "       '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0',\n",
       "       '1', '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '0', '0',\n",
       "       '1', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0',\n",
       "       '0', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0',\n",
       "       '1', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0',\n",
       "       '0', '1', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0',\n",
       "       '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0', '0',\n",
       "       '1', '0', '1', '1', '1', '0', '1', '0', '0', '0', '0', '1', '1',\n",
       "       '1', '1', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0',\n",
       "       '1', '1', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1',\n",
       "       '1', '1', '1', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1',\n",
       "       '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '0', '1', '0', '1', '1', '0', '0', '0', '1', '1', '1', '1',\n",
       "       '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '1',\n",
       "       '0', '1', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0',\n",
       "       '0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '1', '0',\n",
       "       '1', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack(df['embedding'].to_numpy())\n",
    "y = df['label'].to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9510030d-5064-4fb6-b107-f8add103a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.941831381409429"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(200)\n",
    "X_pca = pca.fit_transform(X)\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ed960c7-e451-4767-91f7-3336d4ea45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "799033a2-3179-4c83-9013-40cf5aff7f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 768 features, but LogisticRegression is expecting 200 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/krll5bd51gg0w1t7kc1n3jdr0000gn/T/ipykernel_10032/3825967869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membed_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 768 features, but LogisticRegression is expecting 200 features as input."
     ]
    }
   ],
   "source": [
    "clf.predict([embed_texts(X_train[1][0], model, tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "93497911-bf02-4e46-a711-0e27e7ee16d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24912d19-a7fe-43d5-9926-5ce7efa502bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
